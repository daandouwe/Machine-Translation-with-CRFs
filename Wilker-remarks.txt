WIlker tips:

Regulariser: 
	Use l2 norm
	Should be substraction of regulariser
	(Not working!)

Learning rate:
	o Larger learning rate! Start with 1 or 10
	o Decay the learning rate exponentially at each minibatch step
	  γt = γ0 (1 + γ0*λ*t)**(−1)

Annealing:
	Use BLEU score to determine a weight alpha for each translation. Then sample from a rescaled distribution using this alpha: sample ~p(y,d|x)^alpha which is proportional to exp(alpha * w^T phi(x,y,d)).

	
