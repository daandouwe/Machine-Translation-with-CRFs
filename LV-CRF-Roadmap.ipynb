{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import libitg\n",
    "from libitg import Symbol, Terminal, Nonterminal, Span\n",
    "from libitg import Rule, CFG\n",
    "from libitg import FSA\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook we will discuss what you need to know and do for project 2 :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon\n",
    "\n",
    "I am going to use a very simple lexicon as running example, you would instead use the translation pairs we provided, but note that\n",
    "\n",
    "* we provided you with all translation pairs under IBM1 in both directions, thus there are many pairs in there\n",
    "* you should retain only the top scoring translation pairs (this will make your lexicon smaller and your forests more manageable)\n",
    "* we suggest 5 translations per source word\n",
    "* you need to explicitly encode insertion and deletion in your lexicon, you can bootstrap that from alignments to NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = defaultdict(set)\n",
    "lexicon['le'].update(['the', '-EPS-'])  # we will assume that `le` can be deleted\n",
    "lexicon['-EPS-'].update(['a', 'the'])  # we will assume that `the` and `a` can be inserted\n",
    "lexicon['e'].add('and')\n",
    "lexicon['chien'].add('dog')\n",
    "# lexicon['chien'].add('cat')\n",
    "lexicon['noir'].update(['black', 'dark'])  \n",
    "lexicon['blanc'].add('white')\n",
    "lexicon['petit'].update(['small', 'little'])\n",
    "lexicon['petite'].update(['small', 'little'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with the parser\n",
    "\n",
    "You should play with the basic data structures and algorithms we provided you with, namely\n",
    "\n",
    "* Symbol: Terminal, Nonterminal, Span\n",
    "* Rule\n",
    "* CFG\n",
    "* FSA\n",
    "* ITG-related functions\n",
    "* earley parser\n",
    "\n",
    "you need to know what they do and when to use them to obtain something you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITG\n",
    "\n",
    "We deal with ITGs by\n",
    "\n",
    "* constructing a source-language CFG\n",
    "* parsing a source string\n",
    "* projecting the resulting forest onto the target-language vocabulary through the rules of an ITG and the lexicon\n",
    "\n",
    "\n",
    "**Note on performance:** we could in principle instantiate a different source-side CFG for each source sentence by constraining the lexicon to source words that occur in the sentence we are translating.\n",
    "That is to say that if our sentence does not contain a word (e.g. *petite*) there is no point in including that word in the CFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function uses the entire lexicon\n",
    "src_cfg = libitg.make_source_side_itg(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| 'le'\n",
      "[X] ||| '-EPS-'\n",
      "[X] ||| 'e'\n",
      "[X] ||| 'chien'\n",
      "[X] ||| 'noir'\n",
      "[X] ||| 'blanc'\n",
      "[X] ||| 'petit'\n",
      "[X] ||| 'petite'\n"
     ]
    }
   ],
   "source": [
    "print(src_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSA\n",
    "\n",
    "We represent a sentence as a linear-chain FSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=le\n",
      "origin=1 destination=2 label=chien\n",
      "origin=2 destination=3 label=noir\n"
     ]
    }
   ],
   "source": [
    "src_fsa = libitg.make_fsa('le chien noir')\n",
    "print(src_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forests\n",
    "\n",
    "Forests represent sets of derivations, they are represented by CFGs where symbols have been decorated with spans.\n",
    "We are working with Earley parser, which takes a CFG, an FSA, the CFG's starting symbol, and the symbol that should be used as the starting symbol of the resulting CFG.\n",
    "Note that Earley takes a CFG and returns another!\n",
    "\n",
    "The most basic forest we need is one that contains all derivations of the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| 'le':0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-0 ||| '-EPS-':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:3-3 ||| '-EPS-':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:1-2 ||| 'chien':1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:1-1 ||| '-EPS-':1-1\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[X]:2-2 ||| '-EPS-':2-2\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:2-3 ||| 'noir':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[D(x)] ||| [S]:0-3\n"
     ]
    }
   ],
   "source": [
    "forest = libitg.earley(src_cfg, src_fsa, \n",
    "                       start_symbol=Nonterminal('S'), \n",
    "                       sprime_symbol=Nonterminal(\"D(x)\"))\n",
    "print(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to project this forest to the target vocabulary by using the lexicon and the ITG template rules.\n",
    "This is precisely the set \\\\(\\mathcal D(x)\\\\) of our lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:3-3 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:2-3 [X]:0-2\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:1-3 [X]:0-1\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:0-0\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| [X]:1-1 [X]:0-1\n",
      "[X]:0-1 ||| 'the':0-1\n",
      "[X]:0-1 ||| '-EPS-':0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:0-0\n",
      "[X]:0-0 ||| 'a':0-0\n",
      "[X]:0-0 ||| 'the':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:2-2 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:1-2 [X]:0-1\n",
      "[X]:3-3 ||| 'a':3-3\n",
      "[X]:3-3 ||| 'the':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:1-2 ||| 'dog':1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:1-1\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| [X]:2-2 [X]:1-2\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:1-1 ||| 'a':1-1\n",
      "[X]:1-1 ||| 'the':1-1\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:1-3 ||| [X]:2-3 [X]:1-2\n",
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:1-1\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:1-3 ||| [X]:3-3 [X]:1-3\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[X]:2-2 ||| 'a':2-2\n",
      "[X]:2-2 ||| 'the':2-2\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:2-2\n",
      "[X]:2-3 ||| 'black':2-3\n",
      "[X]:2-3 ||| 'dark':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[X]:2-3 ||| [X]:3-3 [X]:2-3\n",
      "[D(x)] ||| [S]:0-3\n"
     ]
    }
   ],
   "source": [
    "projected_forest = libitg.make_target_side_itg(forest, lexicon)\n",
    "print(projected_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with a translation observation\n",
    "\n",
    "In training, we can observe translations, thus we need to constrain \\\\(\\mathcal D(x)\\\\) to the set of derivations that in addition to \\\\(x\\\\) also produce our target observation \\\\(y\\\\).\n",
    "\n",
    "We do that by intersecting the forest for \\\\(\\mathcal D(x)\\\\) with an automaton that represents \\\\(y\\\\).\n",
    "\n",
    "That is, we first make an FSA for the target sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=the\n",
      "origin=1 destination=2 label=black\n",
      "origin=2 destination=3 label=dog\n"
     ]
    }
   ],
   "source": [
    "tgt_fsa = libitg.make_fsa('the black dog')\n",
    "print(tgt_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then parse this FSA with the projected forest we got earlier from Earley.\n",
    "The resulting forest will represent the set \\\\(\\mathcal D(x,y)\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[D(x,y)] ||| [D(x)]:0-3\n"
     ]
    }
   ],
   "source": [
    "ref_forest = libitg.earley(projected_forest, tgt_fsa, \n",
    "                           start_symbol=Nonterminal(\"D(x)\"), \n",
    "                           sprime_symbol=Nonterminal('D(x,y)'))\n",
    "print(ref_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the black dog'}"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just to illustrate that ref_forest accepts a single target (English) string\n",
    "ref_forest_as_fsa = libitg.forest_to_fsa(ref_forest, Nonterminal('D(x,y)'))\n",
    "libitg.enumerate_paths_in_fsa(ref_forest_as_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length constraint\n",
    "\n",
    "In order to learn our CRF for maximum likelihood we need to compute the gradient of the log-likelihood.\n",
    "\n",
    "The gradient invoves two expectations (see lecture notes and project description). One expectation summarises all ways in which we can derive the joint observation \\\\((x, y)\\\\), the other summarises all ways in which we can derive the incomplete observation \\\\(x, n\\\\). The former is computed out of the forest that represents \\\\(\\mathcal D(x, y)\\\\) and the latter is computed out of the forest that represents \\\\(\\mathcal D_n(x)\\\\).\n",
    "\n",
    "Here we will show you how to get \\\\(\\mathcal D_n(x)\\\\) by parsing a special automaton that accepts the language \\\\(\\Delta^n\\\\) where \\\\(\\Delta\\\\) is the vocabulary of the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_fsa = libitg.LengthConstraint(4, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This special automaton accepts strings containing 1 to 4 words, no matter which words. The label -WILDCARD- is a special label such that `x == -WILDCARD-` evaluates to `True` no matter the value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=5\n",
      "initial=0\n",
      "final=1 2 3 4\n",
      "arcs=4\n",
      "origin=0 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-WILDCARD-\n",
      "origin=3 destination=4 label=-WILDCARD-\n"
     ]
    }
   ],
   "source": [
    "print(length_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D(x)]:0-4 ||| [S]:0-3:0-4\n",
      "[D(x)]:0-2 ||| [S]:0-3:0-2\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[S]:0-3:0-2 ||| [X]:0-3:0-2\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[S]:0-3:0-4 ||| [X]:0-3:0-4\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-2 [X]:0-1:2-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-1 [X]:0-2:1-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-1 [X]:2-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-2 [X]:2-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:0-0:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:3-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:3-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-2 [X]:0-2:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:0-0:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-1 [X]:2-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-2 [X]:0-1:2-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-3 [X]:0-2:3-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-1 [X]:0-2:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:0-0:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-0 [X]:1-3:0-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-4 [X]:0-1:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-2 [X]:2-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-2 [X]:1-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-3 [X]:2-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-3 [X]:0-1:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:3-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-1 [X]:1-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-2 ||| [X]:0-1:0-0 [X]:1-3:0-2\n",
      "[X]:0-3:0-2 ||| [X]:1-3:0-2 [X]:0-1:2-2\n",
      "[X]:0-3:0-2 ||| [X]:2-3:0-1 [X]:0-2:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-2:0-1 [X]:2-3:1-2\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-2 [X]:2-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-1 [X]:1-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:3-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-2 [X]:1-2:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:3-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-1 [X]:1-2:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-3 [X]:2-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-1 [X]:1-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:1-1:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-1 [X]:2-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:1-1:3-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-3 [X]:1-2:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:3-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:1-1:2-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-1 [X]:1-2:1-3\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-2 [X]:2-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-1 [X]:2-3:1-3\n",
      "[X]:1-3:0-2 ||| [X]:2-3:0-1 [X]:1-2:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-2:0-1 [X]:2-3:1-2\n",
      "[X]:0-1:2-3 ||| [X]:1-1:2-3 [X]:0-1:3-3\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:1-1:2-3\n",
      "[X]:0-1:2-3 ||| [X]:0-0:2-3 [X]:0-1:3-3\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:0-0:2-3\n",
      "[X]:0-1:2-3 ||| 'the':0-1:2-3\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:1-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:0-0:2-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:1-1:2-4\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:0-0:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:3-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:2-2:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:2-2:1-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:3-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:2-2:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:2-2:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:3-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:3-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-2 [X]:2-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:2-2:2-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:3-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-2 [X]:2-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:2-2:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:3-3:2-3\n",
      "[X]:2-3:0-1 ||| 'black':2-3:0-1\n",
      "[X]:2-3:0-1 ||| 'dark':2-3:0-1\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-4 [X]:0-1:4-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-2 [X]:0-1:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-3 [X]:1-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:2-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:0-0:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:0-0:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-2 [X]:1-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-3 [X]:0-1:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:2-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-1 [X]:1-2:1-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-3 [X]:0-1:3-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-2 [X]:1-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:2-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-0:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:2-2:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-2 [X]:0-1:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-1 [X]:1-2:1-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:0-0:2-3\n",
      "[X]:0-2:1-2 ||| [X]:1-2:1-2 [X]:0-1:2-2\n",
      "[X]:0-2:1-2 ||| [X]:0-1:1-1 [X]:1-2:1-2\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:0-0:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-0:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-4 [X]:0-1:4-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-3 [X]:1-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-2 [X]:1-2:2-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:2-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:2-2:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-3 [X]:0-1:3-4\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "[X]:3-3:3-4 ||| 'a':3-3:3-4\n",
      "[X]:3-3:3-4 ||| 'the':3-3:3-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-1 [X]:0-0:1-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-3 [X]:0-0:3-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-2 [X]:0-0:2-4\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-1 [X]:0-0:1-3\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-2 [X]:0-0:2-3\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[X]:0-0:0-1 ||| 'a':0-0:0-1\n",
      "[X]:0-0:0-2 ||| [X]:0-0:0-1 [X]:0-0:1-2\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:3-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-4 [X]:0-1:4-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:0-0:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-0:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:3-3:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-2 [X]:0-2:2-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-3 [X]:0-2:3-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-3 [X]:0-1:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-2 [X]:1-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-2 [X]:2-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-1 [X]:1-3:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-3 [X]:2-3:3-4\n",
      "[X]:0-3:1-3 ||| [X]:0-2:1-2 [X]:2-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:0-0:2-3 ||| 'the':0-0:2-3\n",
      "[X]:0-0:2-3 ||| 'a':0-0:2-3\n",
      "[X]:0-0:2-4 ||| [X]:0-0:2-3 [X]:0-0:3-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:0-0:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:0-0:0-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:1-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:1-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:0-0:3-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:1-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:0-0:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:1-1:0-4\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-1 [X]:0-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:0-0:1-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:0-0:0-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:1-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-1 [X]:0-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:1-1:0-2\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:0-0:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:0-0:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-1 [X]:0-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:0-0:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:1-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:1-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-1 [X]:0-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:1-1:0-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-2:1-2 [X]:2-3:2-3\n",
      "[X]:1-3:1-4 ||| [X]:3-3:1-2 [X]:1-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-1:1-2 [X]:1-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:3-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-2 [X]:1-2:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:1-1:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-2 [X]:2-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-3 [X]:1-2:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-3 [X]:2-3:3-4\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-1 [X]:0-1:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:0-0:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:2-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-0 [X]:1-2:0-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-1 [X]:1-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:2-2:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-0:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-2 [X]:0-1:2-2\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-0 [X]:1-2:0-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-3 [X]:0-1:3-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:2-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-1 [X]:0-1:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-2 [X]:1-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:0-0:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:0-0:2-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-1 [X]:1-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-2 [X]:0-1:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:2-2:1-3\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:0-0:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-1 [X]:1-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:2-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-2 [X]:1-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-4 [X]:0-1:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:0-0:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:2-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-3 [X]:1-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:2-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-2 [X]:0-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-1 [X]:0-1:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-0 [X]:1-2:0-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-2 [X]:0-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-2 [X]:0-1:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:0-0:3-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-3 [X]:0-1:3-4\n",
      "[X]:0-2:0-1 ||| [X]:1-2:0-1 [X]:0-1:1-1\n",
      "[X]:0-2:0-1 ||| [X]:0-1:0-0 [X]:1-2:0-1\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:3-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:3-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:2-2:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:2-2:3-4\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-2:1-2 [X]:2-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:2-2:2-3\n",
      "[X]:2-3:1-3 ||| [X]:3-3:1-2 [X]:2-3:2-3\n",
      "[X]:2-3:1-2 ||| 'dark':2-3:1-2\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:2-3:2-3 ||| 'dark':2-3:2-3\n",
      "[X]:2-3:2-3 ||| 'black':2-3:2-3\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:3-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-2:2-3 [X]:2-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:2-2:3-4\n",
      "[X]:2-3:2-4 ||| [X]:3-3:2-3 [X]:2-3:3-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-3 [X]:3-3:3-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-1 [X]:3-3:1-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-2 [X]:3-3:2-4\n",
      "[X]:3-3:0-1 ||| 'a':3-3:0-1\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:3-3:0-2 ||| [X]:3-3:0-1 [X]:3-3:1-2\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-2 [X]:3-3:2-3\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-1 [X]:3-3:1-3\n",
      "[X]:0-3:2-4 ||| [X]:0-2:2-3 [X]:2-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:2-3:2-3 [X]:0-2:3-4\n",
      "[X]:0-3:2-4 ||| [X]:1-3:2-4 [X]:0-1:4-4\n",
      "[X]:0-3:2-4 ||| [X]:0-1:2-2 [X]:1-3:2-4\n",
      "[X]:0-2:3-4 ||| [X]:1-2:3-4 [X]:0-1:4-4\n",
      "[X]:0-2:3-4 ||| [X]:0-1:3-3 [X]:1-2:3-4\n",
      "[X]:0-0:3-4 ||| 'the':0-0:3-4\n",
      "[X]:0-0:3-4 ||| 'a':0-0:3-4\n",
      "[X]:0-1:4-4 ||| '-EPS-':0-1:4-4\n",
      "[X]:3-3:2-3 ||| 'a':3-3:2-3\n",
      "[X]:3-3:2-3 ||| 'the':3-3:2-3\n",
      "[X]:3-3:2-4 ||| [X]:3-3:2-3 [X]:3-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:1-2:2-3 [X]:2-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:2-3:2-3 [X]:1-2:3-4\n",
      "[X]:0-1:3-4 ||| [X]:0-0:3-4 [X]:0-1:4-4\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:1-1:3-4\n",
      "[X]:0-1:3-4 ||| [X]:1-1:3-4 [X]:0-1:4-4\n",
      "[X]:0-1:3-4 ||| 'the':0-1:3-4\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:0-0:3-4\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[X]:2-3:3-4 ||| 'dark':2-3:3-4\n",
      "[X]:2-3:3-4 ||| 'black':2-3:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:1-1:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:2-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:2-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:1-1:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:2-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:1-1:1-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:1-1:2-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:1-1:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:2-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:2-2:2-3\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:1-1:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-1:0-1 [X]:1-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:2-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:2-2:0-1 [X]:1-2:1-2\n",
      "[X]:1-2:0-1 ||| 'dog':1-2:0-1\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:2-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:1-1:3-4\n",
      "[X]:1-2:2-4 ||| [X]:2-2:2-3 [X]:1-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-1:2-3 [X]:1-2:3-4\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-3 [X]:1-1:3-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-1 [X]:1-1:1-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-2 [X]:1-1:2-4\n",
      "[X]:1-1:0-1 ||| 'a':1-1:0-1\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:1-1:0-2 ||| [X]:1-1:0-1 [X]:1-1:1-2\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-2 [X]:1-1:2-3\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-1 [X]:1-1:1-3\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:2-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:1-1:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:1-1:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:2-2:2-4\n",
      "[X]:1-2:1-2 ||| 'dog':1-2:1-2\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:1-1:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:2-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-1:1-2 [X]:1-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:2-2:1-2 [X]:1-2:2-3\n",
      "[X]:1-1:2-3 ||| 'a':1-1:2-3\n",
      "[X]:1-1:2-3 ||| 'the':1-1:2-3\n",
      "[X]:1-1:2-4 ||| [X]:1-1:2-3 [X]:1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'a':1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'the':1-1:3-4\n",
      "[X]:1-2:3-4 ||| 'dog':1-2:3-4\n",
      "[X]:3-3:1-2 ||| 'a':3-3:1-2\n",
      "[X]:3-3:1-2 ||| 'the':3-3:1-2\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-2 [X]:3-3:2-4\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-3 [X]:3-3:3-4\n",
      "[X]:3-3:1-3 ||| [X]:3-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-2:1-3 ||| [X]:2-2:1-2 [X]:2-2:2-3\n",
      "[X]:2-2:1-2 ||| 'a':2-2:1-2\n",
      "[X]:2-2:1-2 ||| 'the':2-2:1-2\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-2 [X]:2-2:2-4\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-3 [X]:2-2:3-4\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-1 [X]:2-2:1-3\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-2 [X]:2-2:2-3\n",
      "[X]:2-2:0-1 ||| 'a':2-2:0-1\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[X]:2-2:0-2 ||| [X]:2-2:0-1 [X]:2-2:1-2\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-1 [X]:2-2:1-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-2 [X]:2-2:2-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-3 [X]:2-2:3-4\n",
      "[X]:2-2:2-4 ||| [X]:2-2:2-3 [X]:2-2:3-4\n",
      "[X]:2-2:2-3 ||| 'the':2-2:2-3\n",
      "[X]:2-2:2-3 ||| 'a':2-2:2-3\n",
      "[X]:2-2:3-4 ||| 'the':2-2:3-4\n",
      "[X]:2-2:3-4 ||| 'a':2-2:3-4\n",
      "[X]:0-1:1-2 ||| [X]:1-1:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:0-0:1-2\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:1-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:0-0:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| 'the':0-1:1-2\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-3 [X]:0-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:1-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:0-0:1-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:1-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:0-0:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:0-0:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:1-1:1-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-3 [X]:0-1:3-4\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:1-1:1-3\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:1-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:0-0:1-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:0-0:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-2 [X]:0-0:2-4\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-3 [X]:0-0:3-4\n",
      "[X]:0-0:1-3 ||| [X]:0-0:1-2 [X]:0-0:2-3\n",
      "[X]:0-0:1-2 ||| 'the':0-0:1-2\n",
      "[X]:0-0:1-2 ||| 'a':0-0:1-2\n",
      "[X]:1-1:1-2 ||| 'a':1-1:1-2\n",
      "[X]:1-1:1-2 ||| 'the':1-1:1-2\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-2 [X]:1-1:2-4\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-3 [X]:1-1:3-4\n",
      "[X]:1-1:1-3 ||| [X]:1-1:1-2 [X]:1-1:2-3\n",
      "[D_n(x)] ||| [D(x)]:0-4\n",
      "[D_n(x)] ||| [D(x)]:0-2\n",
      "[D_n(x)] ||| [D(x)]:0-3\n"
     ]
    }
   ],
   "source": [
    "target_forest = libitg.earley(projected_forest, length_fsa, \n",
    "                              start_symbol=Nonterminal(\"D(x)\"), \n",
    "                              sprime_symbol=Nonterminal(\"D_n(x)\"))\n",
    "print(target_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this produces a super large FSA which enumerates the strings in the forest\n",
    "target_forest_as_fsa = libitg.forest_to_fsa(target_forest, Nonterminal('D_n(x)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black\n",
      "a black dog\n",
      "a dark\n",
      "a dark dog\n",
      "a dog\n",
      "a dog black\n",
      "a dog dark\n",
      "black a\n",
      "black a dog\n",
      "black dog\n",
      "black dog a\n",
      "black dog the\n",
      "black the\n",
      "black the dog\n",
      "dark a\n",
      "dark a dog\n",
      "dark dog\n",
      "dark dog a\n",
      "dark dog the\n",
      "dark the\n",
      "dark the dog\n",
      "dog a\n",
      "dog a black\n",
      "dog a dark\n",
      "dog black\n",
      "dog black a\n",
      "dog black the\n",
      "dog dark\n",
      "dog dark a\n",
      "dog dark the\n",
      "dog the\n",
      "dog the black\n",
      "dog the dark\n",
      "the black\n",
      "the black dog\n",
      "the dark\n",
      "the dark dog\n",
      "the dog\n",
      "the dog black\n",
      "the dog dark\n"
     ]
    }
   ],
   "source": [
    "# here we get the strings in a normal python set\n",
    "candidates = libitg.enumerate_paths_in_fsa(target_forest_as_fsa)\n",
    "for candidate in sorted(candidates):\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings in D_n(x): 40\n",
      "Does D_n(x) contain the reference? yes\n"
     ]
    }
   ],
   "source": [
    "# Let's check how many string we got\n",
    "print('Strings in D_n(x): %d' % len(candidates))\n",
    "# and check whether the gold-standard reference is still in the set of candidates (it should be!)\n",
    "print('Does D_n(x) contain the reference? %s' % ('yes' if 'the black dog' in candidates else 'no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Earley parsing helped us instatiated the sets which will be support to our probability distribution, in particular,\n",
    "\n",
    "* \\\\(\\mathcal D_n(x)\\\\) will be support to our joint distribution \\\\(P(Y, D|X=x, N=n)\\\\)\n",
    "* \\\\(\\mathcal D(x, y)\\\\) will be support to our posterior distribution \\\\(P(D|X=x, Y=y, N=n)\\\\)\n",
    "\n",
    "and our joint distribution corresponds to \n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{y' \\in \\mathcal Y_n(x)} \\sum_{d' \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y', d', x, n))} \\\\\n",
    " &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{d' \\in \\mathcal D_n(x)} \\exp(w^\\top \\Phi(y', d', x, n))}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "* \\\\(\\Phi\\\\) is a feature function that maps any tuple \\\\(y, d, x, n\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\theta\\\\) is a weight vector in \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\mathcal Y_n(x)\\\\) is the set of translations of \\\\(x\\\\) that are at most \\\\(n\\\\)-words long\n",
    "* and the last equality is due to the fact that there is a deterministic mapping between a derivation and the string it projects on either language, that is, \\\\(y' = \\text{yield}_\\Delta(d')\\\\).\n",
    "\n",
    "In order to instantiate the joint distribution for the entire space \\\\(\\mathcal D_n(x)\\\\) we must use features that can be locally assigned to the steps of a derivation. This corresponds to the idea of having *local features* or *edge potentials*:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) \n",
    " &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{\\sum_{r_{s, t} \\in d' \\in \\mathcal D_n(x)} \\exp\\left( \\sum_c w^\\top \\phi(r, s, t, x, n) \\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* a derivation is seen as a sequence of rules decorated with spans\n",
    "* \\\\(r_{s,t}\\\\) is a rule decorated with a source and a target span (in the code sometimes we call this an *edge*)\n",
    "* \\\\(\\phi\\\\) is a (local) feature function that maps any tuple \\\\((r, s, t, x, n)\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* note that \\\\(\\Phi(y, d, x, n) = \\sum_{r_{s, t} \\in d} \\phi(r, s, t, x, n)\\\\)\n",
    "\n",
    "The formulation in terms of local potentials make it clear that in designing feature representations for derivations we have unrestricted access to \\\\(x\\\\), \\\\(n\\\\), the rule identiy \\\\(r\\\\), the source span \\\\(s\\\\) and the target span \\\\(t\\\\), but note that because we do not have unrestricted access to \\\\(y\\\\), the information that \\\\(t\\\\) provides is rather limited.\n",
    "\n",
    "* For example, knowing the rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` tells us that the LHS spans from 1 to 3 on the source. Suppose, we are translating \\\\(x\\\\) = `le chien noir`, then we know the the LHS projects onto `chien noir`, that it is prefixed by `BOS le` and followed by `EOS` (end-of-sentence). On the other hand, if we focus on the target span, we only know that we will produce up 2 words (because `3-1=2`) but we dont't know which words those are;\n",
    "* Another rule, `X_{2-3,1-2} -> noir/black` tells us a different story, it tells us that the lexical entry `noir` has been aligned to the lexical entry `black`, it also tell us about the positions where these words are, so we could for example compute a distortion feature similar to IBM2's jump value;\n",
    "* A rule, `X_{0-1,0-0} -> le/-EPS-` tells us that a deletion has happened;\n",
    "* Insertions can for example appear like this `X_{1-2,1-3} -> X_{1-1,1-2} X_{1-2,2-3}`, note that the first RHS symbol has an empty source span `1-1`, but the target span is longer than 0, this means an insertion has happened;\n",
    "* The rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` also tells us that we are making an inversion, note that the source spans are not adjacent, but rather flipped, e.g. 2-3 for the first RHS symbol and 1-2 for the second RHS symbol, we could have a feature capture that;\n",
    "\n",
    "After undestanding how to use the parser to get the relevant sets, you should focus on implement a feature function that can convert the information available on an edge into a sparse feature vector.\n",
    "\n",
    "We suggest you start with a very simple feature function that is mostly *dense*. This will make development a lot easier in the beginning. Here we show a minimal version of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some auxiliary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_terminal_string(symbol: Symbol):\n",
    "    \"\"\"Returns the python string underlying a certain terminal (thus unwrapping all span annotations)\"\"\"\n",
    "    if not symbol.is_terminal():\n",
    "        raise ValueError('I need a terminal, got %s of type %s' % (symbol, type(symbol)))\n",
    "    return symbol.root().obj()\n",
    "\n",
    "def get_bispans(symbol: Span):\n",
    "    \"\"\"\n",
    "    Returns the bispans associated with a symbol. \n",
    "    \n",
    "    The first span returned corresponds to paths in the source FSA (typically a span in the source sentence),\n",
    "     the second span returned corresponds to either\n",
    "        a) paths in the target FSA (typically a span in the target sentence)\n",
    "        or b) paths in the length FSA\n",
    "    depending on the forest where this symbol comes from.\n",
    "    \"\"\"\n",
    "    if not isinstance(symbol, Span):\n",
    "        raise ValueError('I need a span, got %s of type %s' % (symbol, type(symbol)))\n",
    "    s, start2, end2 = symbol.obj()  # this unwraps the target or length annotation\n",
    "    _, start1, end1 = s.obj()  # this unwraps the source annotation\n",
    "    return (start1, end1), (start2, end2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-3:0-2 ||| [X]:0-1:0-0 [X]:1-3:0-2\n",
      "[X]:0-3:0-2\n",
      "True\n",
      "((0, 3), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "span = list(target_forest.items())[8]\n",
    "symbol = span[0]\n",
    "rule4 = span[1][0]\n",
    "print(rule4)\n",
    "print(symbol)\n",
    "print(type(symbol.obj()[0])==Span)\n",
    "print(get_bispans(symbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define our prototype feature function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_features(edge: Rule, src_fsa: FSA, eps=Terminal('-EPS-'), \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False,\n",
    "                   ch_en=defaultdict(float), en_ch=defaultdict(float)) -> dict:\n",
    "    \"\"\"\n",
    "    Featurises an edge given\n",
    "        * rule and spans\n",
    "        * src sentence as an FSA\n",
    "        * TODO: target sentence length n\n",
    "        * TODO: extract IBM1 dense features\n",
    "    crucially, note that the target sentence y is not available!    \n",
    "    \"\"\"\n",
    "    fmap = defaultdict(float)\n",
    "    fset = set() # stores the features we've added\n",
    "    if len(edge.rhs) == 2:  # binary rule\n",
    "        fmap['type:binary'] += 1.0\n",
    "        fset.add('type:binary')\n",
    "        # here we could have sparse features of the source string as a function of spans being concatenated\n",
    "        (ls1, ls2), (lt1, lt2) = get_bispans(edge.rhs[0])  # left of RHS\n",
    "        (rs1, rs2), (rt1, rt2) = get_bispans(edge.rhs[1])  # right of RHS        \n",
    "        \n",
    "        # TODO: double check these, assign features, add some more\n",
    "        if ls1 == ls2:  # deletion of source left child\n",
    "            fmap['type:deletion-slc'] += 1.0\n",
    "            fset.add('type:deletion-slc')\n",
    "        if rs1 == rs2:  # deletion of source right child\n",
    "            fmap['type:deletion-src'] += 1.0\n",
    "            fset.add('type:deletion-src')\n",
    "        if ls2 == rs1:  # monotone\n",
    "            fmap['type:monotone'] += 1.0\n",
    "            fset.add('type:monotone')\n",
    "        if ls1 == rs2:  # inverted\n",
    "            fmap['type:inverted'] += 1.0\n",
    "            fset.add('type:inverted')\n",
    "            \n",
    "        \n",
    "        # source span feature of rhs\n",
    "        src_span_lc = ls2 - ls1\n",
    "        src_span_rc = rs2 - rs1\n",
    "        fmap['span:rhs:src-lc:{}'.format(src_span_lc)] += 1.0\n",
    "        fmap['span:rhs:src-rc:{}'.format(src_span_rc)] += 1.0\n",
    "        fset.update({'span:rhs:src-lc:{}'.format(src_span_lc),\n",
    "                     'span:rhs:src-rc:{}'.format(src_span_rc)})\n",
    "        # target span feature of rhs\n",
    "        tgt_span_lc = lt2 - lt1\n",
    "        tgt_span_rc = rt2 - rt1\n",
    "        fmap['span:rhs:tgt-lc:{}'.format(tgt_span_lc)] += 1.0\n",
    "        fmap['span:rhs:tgt-rc:{}'.format(tgt_span_rc)] += 1.0\n",
    "        fset.update({'span:rhs:tgt-lc:{}'.format(tgt_span_lc),\n",
    "                     'span:rhs:tgt-rc:{}'.format(tgt_span_rc)})\n",
    "        \n",
    "    else:  # unary\n",
    "        symbol = edge.rhs[0]\n",
    "        if symbol.is_terminal():  # terminal rule\n",
    "            fmap['type:terminal'] += 1.0\n",
    "            fset.add('type:terminal')\n",
    "            # we could have IBM1 log probs for the traslation pair or ins/del\n",
    "            (s1, s2), (t1, t2) = get_bispans(symbol)\n",
    "            src_word = src_fsa.label(s1, s2)\n",
    "            tgt_word = get_terminal_string(symbol)\n",
    "            if symbol.root() == eps:  # symbol.root() gives us a Terminal free of annotation\n",
    "                fmap['type:deletion'] += 1.0\n",
    "                fset.add('type:deletion')\n",
    "                # dense versions (for initial development phase)\n",
    "#                 fmap['ibm1:del:logprob'] += np.log(en_ch[src_word]['<NULL>'])\n",
    "                \n",
    "                # sparse version\n",
    "                if sparse_del:\n",
    "                    fmap['del:%s' % src_word] += 1.0\n",
    "                    fset.add('del:%s' % src_word)\n",
    "            else:                \n",
    "                if s1 == s2:  # has not consumed any source word, must be an eps rule\n",
    "                    fmap['type:insertion'] += 1.0\n",
    "                    fset.add('type:insertion')\n",
    "                    # dense version\n",
    "#                     fmap['ibm1:ins:logprob'] += np.log(ch_en['<NULL>'][tgt_word])\n",
    "                    \n",
    "                    # sparse version\n",
    "                    if sparse_ins:\n",
    "                        fmap['ins:%s' % tgt_word] += 1.0\n",
    "                        fset.add('ins:%s' % tgt_word)\n",
    "                else:\n",
    "                    fmap['type:translation'] += 1.0\n",
    "                    fset.add('type:translation')\n",
    "                    # dense version\n",
    "#                     fmap['ibm1:x2y:logprob'] += np.log(ch_en[src_word][tgt_word])  # y is english word \n",
    "#                     fmap['ibm1:y2x:logprob'] += np.log(en_ch[tgt_word][src_word])\n",
    "                    \n",
    "                    # sparse version                    \n",
    "                    if sparse_trans:\n",
    "                        fmap['trans:%s/%s' % (src_word, tgt_word)] += 1.0\n",
    "                        fset.add('trans:%s/%s' % (src_word, tgt_word))\n",
    "        \n",
    "            # source span feature of rhs\n",
    "            src_span = s2 - s1\n",
    "            fmap['span:rhs:src:{}'.format(src_span)] += 1.0\n",
    "            fset.add('span:rhs:src:{}'.format(src_span))\n",
    "            # target span feature of rhs\n",
    "            tgt_span = t2 - t1\n",
    "            fmap['span:rhs:tgt:{}'.format(tgt_span)] += 1.0\n",
    "            fset.add('span:rhs:tgt:{}'.format(tgt_span))\n",
    "                \n",
    "        else:  # S -> X\n",
    "            fmap['top'] += 1.0\n",
    "            fset.add('top')\n",
    "\n",
    "        # bispans of lhs of edge for source and target (source and target sentence lengths)\n",
    "        if isinstance(edge.lhs.obj()[0], Span): # exclude the (Nonterminal('D(x)'), 0, 2) rules\n",
    "            (s1, s2), (t1, t2) = get_bispans(edge.lhs)\n",
    "            # source span feature of lhs\n",
    "            src_span = s2 - s1\n",
    "            fmap['span:lhs:src:{}'.format(src_span)] += 1.0\n",
    "            fset.add('span:lhs:src:{}'.format(src_span))\n",
    "            # target span feature of lhs\n",
    "            tgt_span = t2 - t1\n",
    "            fmap['span:lhs:tgt:{}'.format(tgt_span)] += 1.0\n",
    "            fset.add('span:lhs:tgt:{}'.format(tgt_span))\n",
    "\n",
    "    return fmap, fset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply featurize edges, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_edges(forest, src_fsa, \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False,\n",
    "                    eps=Terminal('-EPS-')) -> dict:\n",
    "    edge2fmap = defaultdict(float)\n",
    "    fset_accum = set()\n",
    "    for edge in forest:\n",
    "        edge2fmap[edge], fset = simple_features(edge, src_fsa, eps, sparse_del, sparse_ins, sparse_trans)\n",
    "        fset_accum.update(fset)\n",
    "    return edge2fmap, fset_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge2fmap, fset = featurize_edges(target_forest, src_fsa,\n",
    "                                        sparse_del=True, sparse_ins=True, sparse_trans=True)\n",
    "\n",
    "# print('\\n'.join(sorted(list(fset))))\n",
    "# # seeing features at work:\n",
    "# for edge in target_forest:\n",
    "#     print(edge)\n",
    "#     for fmap in edge2fmap[edge]:\n",
    "#         print('\\t{}'.format(fmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recall the definition of the joint distribution\n",
    "\\begin{align}\n",
    "P(y,d|x, n) &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{Z_n(x)}\n",
    "\\end{align}\n",
    "for convenience, let's work with log probabilities, \n",
    "\\begin{align}\n",
    "\\log P(y,d|x,n) &= \\log \\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right) - \\log Z_n(x) \\\\\n",
    "  &= \\underbrace{\\sum_{r_{s,t} \\in d} \\underbrace{w^\\top \\phi(r, s, t, x, n)}_{\\text{local log-potential}}}_{\\text{log of unnormalised distribution}} - \\log Z_n(x)\n",
    "\\end{align}\n",
    "Now, in the log-domain, we can compute the logarithm of unnormalised probabilities by summing along the edges in a derivation.\n",
    "\n",
    "This is great news! We can express the log of the unnormalised distribution by decorating edges with local log-potentials, that is, \n",
    "\\begin{align}\n",
    "\\omega(r_{s,t}) = w^\\top \\phi(r, s, t, x, n)\n",
    "\\end{align}\n",
    "where \\\\(\\omega\\\\) takes an edge in the hypergraph and returns its local log-potential (dot product of feature weights and feature vector).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to define a weight function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_function(edge, fmap, wmap) -> float:\n",
    "    # dot product of fmap and wmap  (working in log-domain)\n",
    "    dot = 0.0\n",
    "    for feature, value in fmap.items():\n",
    "        dot += value * wmap[feature]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial feature map has value 1.0 for all features\n",
    "# wmap = {feature: 0.01 for feature in fset}\n",
    "wmap = defaultdict(float)\n",
    "for feature in fset:\n",
    "    wmap[feature] = 0.01\n",
    "\n",
    "edge_weights = defaultdict(float)\n",
    "\n",
    "# seeing the dot prodcut at work\n",
    "# for edge in target_forest:\n",
    "#     print(edge)\n",
    "#     fmap = edge2fmap[edge]\n",
    "#     dot = weight_function(edge, fmap, wmap)\n",
    "#     edge_weights[edge] = dot\n",
    "#     print('\\t{}'.format(dot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "Working with hypergraphs can be tedious, they can be quite large, a few tips:\n",
    "\n",
    "* do not parse very long sentences: discard training instances where either string is longer than say 10 words in development phase, for the final report you can try to stretch all the way to 30 or so;\n",
    "* parse once: with a fixed lexicon and grammar, the parse forests will never change, thus produce them one at a time and pickle them to disk, so that whenever you need the forest during SGD it will be pre-computed\n",
    "* featurise once: same applies to feature vectors, if you do not change your feature function, feature vectors will remain the same, thus, extract them once and pickle them to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE\n",
    "\n",
    "Now it's time you optimise your likelihood\n",
    "\\begin{align}\n",
    "\\mathcal L(w|x, y) &= \\log P(y|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} P(y, d|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} \\frac{\\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{\\sum_{d \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{Z(x, y)}{Z_n(x)} \\\\\n",
    " &= \\log Z(x, y) - \\log Z_n(x)\n",
    "\\end{align}\n",
    "\n",
    "First of all, we need to efficiently compute\n",
    "* \\\\(\\log Z(x,y)\\\\) the sum of all unnormalised probabilities for derivations that explain the translation pair \\\\(x, y\\\\)\n",
    "* and \\\\(\\log Z_n(x)\\\\) the sum of all unnormalised probabilities for derivations that explain the incomplete observation \\\\(x, n\\\\)\n",
    "now note that we have acyclic hypergraphs whose edges represent log-potentials for a model which is log-linear. This means that we can efficiently compute the global log-normalisers with a recursive formula that visits each node in the forest once. This recursion is called the `Inside algorithm`.\n",
    "\n",
    "The *inside weight* of a node corresponds to the sum of the weights of all paths under this node in the forest.\n",
    "\n",
    "\\begin{equation}\n",
    "    I(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } v \\text{ is terminal }\\\\\n",
    "        \\bigoplus_{e \\in BS(v)} \\omega(e) \\bigotimes_{u \\in tail(e)} I(u) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(u\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{tail}(e)\\\\) is a sequence of children nodes (this is the RHS of an annotated rule in the forest)\n",
    "* \\\\(BS(v)\\\\) corresponds to the set of edges that are incoming to v (this is the set of annotated rules for which v is LHS in the forest)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\otimes\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "The inside recursion can be implemented iteratively by visiting the nodes in [top-sorted order](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "              \n",
    "Check the lecture notes and [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseudo-code of the Inside algorithm.\n",
    "\n",
    "The inside at the root of the forest represents the log-normaliser of the forest.\n",
    "\n",
    "Now you should be ready to implement topsort and the inside algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_sort(forest: CFG) -> list:\n",
    "    \"\"\"Returns ordered list of nodes according to topsort order in an acyclic forest\"\"\"\n",
    "    S = {symbol for symbol in forest.terminals} # (Copy!) only terminals have no dependecies\n",
    "    D = {symbol: {child for rule in forest.get(symbol) for child in rule.rhs}\\\n",
    "                 for symbol in forest.nonterminals|forest.terminals} # forest.nonterminals|forest.terminals = V\n",
    "    L = list()\n",
    "    while S: # while S nonempty\n",
    "        u = S.pop()\n",
    "        L.append(u)\n",
    "        outgoing = [e for e in forest if u in e.rhs] # outgoing = FS(u)\n",
    "        for rule in outgoing:\n",
    "            v = rule.lhs\n",
    "            D[v] = D[v] - {u}\n",
    "            if len(D[v]) == 0:\n",
    "                S = S | {v}\n",
    "    return L\n",
    "\n",
    "def inside_algorithm(forest: CFG, tsort: list, edge_weights: dict) -> dict:\n",
    "    \"\"\"Returns the inside weight of each node\"\"\"\n",
    "    I = dict()\n",
    "    for symbol in tsort: # symbol is v\n",
    "        incoming = forest.get(symbol) # BS(v) - gets all the incoming nodes, i.e. all rules where symbol is lhs\n",
    "        if len(incoming) == 0: \n",
    "            I[symbol] = 1.0 # leaves\n",
    "        else:\n",
    "            w = 0.0\n",
    "            for edge in incoming: # edge is of type Rule\n",
    "                k = edge_weights[edge]\n",
    "                for child in edge.rhs: # chid in tail(e)\n",
    "                    k *= I[child] # TODO: change to log-sum-exp\n",
    "                w += k\n",
    "            I[symbol] = w\n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inside at the root of the forest represents the log-normaliser of the forest: `I[root]= `\\\\(\\log Z_n(x)\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4328732999168006e-10\n"
     ]
    }
   ],
   "source": [
    "tsort = top_sort(target_forest)\n",
    "edge_weights = {edge: weight_function(edge, edge2fmap[edge], wmap) for edge in target_forest}\n",
    "\n",
    "I = inside_algorithm(target_forest, tsort, edge_weights)\n",
    "\n",
    "root = Nonterminal(\"D_n(x)\")\n",
    "print(I[root])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we will approach the optimisation of the log-likelihood via SGD, that is, \n",
    "we will do so by taking steps towards the steepest ascent at each training instance (or mini-batch).\n",
    "\n",
    "Taking derivatives with respect to \\\\(w\\\\) we get\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\mathcal L(w|x, y, n) &= \\mathbb E_{P(D|Y=y, X=x, N=n)}[\\Phi(Y, D, X, N)] - \\mathbb E_{P(Y, D|X=x, N=n)}[\\Phi(Y, D, X, N)]\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* the first expectation uses the forest \\\\(\\mathcal D(x, y)\\\\)\n",
    "* the second expectation uses the forest \\\\(\\mathcal D_n(x)\\\\)\n",
    "\n",
    "again because our CRF is edge-factored, we can compute this rather efficiently combining inside weights with *outside* weights, for which we need the *outside algorithm*. If you heard of forward-backward, then inside-outside is just a generalisation for hypergraphs (instead of graphs). You can learn more about expectations from forest from [Li and Eisner](http://www.aclweb.org/anthology/D09-1005).\n",
    "\n",
    "Whereas the inside was defined easily on its own, the outside will be defined differently so that\n",
    "\\begin{align}\n",
    "    I(u) \\otimes O(u)\n",
    "\\end{align}\n",
    "is proportional to the marginal probability of node \\\\(u\\\\), where the missing proportionality constant corresponds to the log-normaliser of the distribution (which is precisely the Inside weight of the root of the forest).\n",
    "\n",
    "\\begin{equation}\n",
    "    O(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } FS(v) =\\emptyset \\\\\n",
    "        \\bigoplus_{e \\in FS(v)} O(\\text{head}(e)) \\bigotimes_{s \\in tail(e)\\setminus\\{v\\}} I(s) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(s\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{head}(e)\\\\) is a the edge's head node (this is the annotated LHS of an annotated rule in the forest)\n",
    "* \\\\(FS(v)\\\\) corresponds to the set of edges that are outgoing from v (this is the set of annotated rules for which v appears in the RHS)\n",
    "* \\\\(I(s)\\\\) is the inside weight of node \\\\(s\\\\)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\otimes\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "\n",
    "Again, check the [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseu-code for the outside algorithm.\n",
    "\n",
    "Now you should be ready to implement the *outside algorithm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outside_algorithm(forest: CFG, tsort: list, edge_weights: dict, inside: dict, root: Symbol) -> dict:\n",
    "    \"\"\"Returns the outside weight of each node\"\"\"\n",
    "    O = dict()\n",
    "    for symbol in tsort:\n",
    "        O[symbol] = 0.0\n",
    "    O[root] = 1.0\n",
    "    for symbol in reversed(tsort):\n",
    "        incoming = forest.get(symbol)\n",
    "        for edge in incoming:\n",
    "            for u in edge.rhs: # u in tail(e)\n",
    "                k = edge_weights[edge] * O[symbol]\n",
    "                for s in edge.rhs:\n",
    "                    if not u == s:\n",
    "                        k *= inside[s] # TODO: change to log-sum-exp\n",
    "                O[u] += k\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "O = outside_algorithm(target_forest, tsort, edge_weights, I, root=Nonterminal(\"D_n(x)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to combine inside and outside and efficiently compute the expected feature vector under each distribution.\n",
    "\n",
    "This takes a linear pass over the edges of the forest, and [Li and Eisner](http://www.aclweb.org/anthology/D09-1005) taught us how to do that very efficiently (see figure 4, or our [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a clean pseudo-code).\n",
    "\n",
    "You should be ready to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expected_feature_vector(forest: CFG, inside: dict, outside: dict, edge_features: dict) -> dict:\n",
    "    \"\"\"Returns an expected feature vector (here a sparse python dictionary)\"\"\"\n",
    "    expected_features = defaultdict(lambda:defaultdict(float))\n",
    "    for rule in forest:\n",
    "        k = outside[rule.lhs]\n",
    "        for symbol in rule.rhs:\n",
    "            k *= inside[symbol]\n",
    "        for feature in edge_features[rule]:\n",
    "            expected_features[rule][feature] = k * edge_features[rule][feature]\n",
    "    return expected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SGD\n",
    "\n",
    "Now it's time to take gradient steps,\n",
    "\n",
    "\\begin{align}\n",
    "    w = w + \\delta \\nabla_w \\mathcal L(w|x, y, n)\n",
    "\\end{align}\n",
    "\n",
    "where for a certain observation \\\\(x, n, y\\\\) we compute the expected feature vectors, subtract them, scale by a learning rate and get a parameter update.\n",
    "If we have a mini-batch, we accumulate the gradients.\n",
    "\n",
    "In the beginning of the project you will be working with the simple (mostly dense) feature function, thus this should work reasonably well. At some point you will model sparse features and then you will most likely need a [regulariser](http://www.aclweb.org/anthology/P08-1024).\n",
    "\n",
    "To diagonose convergence we typically track log-likelihood of training data, with online learning or mini-batching is typically too expensive to go over the entire trainin set computing log-likelihood, thus you can do that on each batch individually and track both the batch log-likelihood and a running average.\n",
    "\n",
    "Lastly, you might want to experiment with averaged parameters where in parallel to the parameters \\\\(w\\\\) that you actually use, you can keep a running average of parameters \\\\(w_{\\text{avg}}\\\\), at then end of learning, this running average typically shows better performance at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from itertools import chain\n",
    "from math import nan\n",
    "\n",
    "def update_w(wmap, expected_features_D_xy, expected_features_Dn_x, delta=0.1):\n",
    "    w_new = defaultdict(float)\n",
    "    nabla_total = 0.0\n",
    "    for rule in chain(expected_features_D_xy, expected_features_Dn_x):\n",
    "        for feature in chain(expected_features_D_xy[rule], expected_features_Dn_x[rule]):\n",
    "            nabla_w = delta * (expected_features_D_xy[rule][feature] - expected_features_Dn_x[rule][feature])\n",
    "            w_new[feature] = wmap[feature] + nabla_w\n",
    "            nabla_total += nabla_w\n",
    "\n",
    "    return w_new, nabla_total\n",
    "\n",
    "def sgd_func(iters, delta, w, target_forest, ref_forest, src_fsa, sparse=False):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent on the weights vector w.\n",
    "    \"\"\"    \n",
    "    for i in range(iters):\n",
    "        \n",
    "        bar = progressbar.ProgressBar(max_value=13)\n",
    "        bar.update(0)\n",
    "        \n",
    "        ### D_n(x) ###\n",
    "        \n",
    "        tgt_edge2fmap, _ = featurize_edges(target_forest, src_fsa,\n",
    "                                           sparse_del=sparse, sparse_ins=sparse, sparse_trans=sparse)\n",
    "        \n",
    "        bar.update(1)\n",
    "\n",
    "        # recompute edge weights\n",
    "        tgt_edge_weights = {edge: np.exp(weight_function(edge, tgt_edge2fmap[edge], w)) \n",
    "                                for edge in target_forest}\n",
    "\n",
    "        bar.update(2)\n",
    "        \n",
    "        # compute inside and outside\n",
    "        tgt_tsort = top_sort(target_forest)\n",
    "        root_tgt = Nonterminal(\"D_n(x)\")\n",
    "        bar.update(3)\n",
    "        I_tgt = inside_algorithm(target_forest, tgt_tsort, tgt_edge_weights)\n",
    "        bar.update(4)\n",
    "        O_tgt = outside_algorithm(target_forest, tgt_tsort, tgt_edge_weights, I_tgt, root_tgt)\n",
    "        bar.update(5)\n",
    "\n",
    "        # compute expected features\n",
    "        expected_features_Dn_x = expected_feature_vector(target_forest, I_tgt, O_tgt, tgt_edge2fmap)\n",
    "        bar.update(6)\n",
    "\n",
    "        \n",
    "        ### D(x,y) ###\n",
    "        \n",
    "        ref_edge2fmap, _ = featurize_edges(ref_forest, src_fsa,\n",
    "                                           sparse_del=sparse, sparse_ins=sparse, sparse_trans=sparse)\n",
    "        bar.update(7)\n",
    "\n",
    "        # recompute edge weights\n",
    "        ref_edge_weights = {edge: np.exp(weight_function(edge, ref_edge2fmap[edge], w))\n",
    "                            for edge in ref_forest}\n",
    "\n",
    "        bar.update(8)\n",
    "        \n",
    "        # compute inside and outside\n",
    "        tsort = top_sort(ref_forest)\n",
    "        root_ref = Nonterminal(\"D(x,y)\")\n",
    "        bar.update(9)\n",
    "        I_ref = inside_algorithm(ref_forest, tsort, ref_edge_weights)\n",
    "        bar.update(10)\n",
    "        O_ref = outside_algorithm(ref_forest, tsort, ref_edge_weights, I_ref, root_ref)\n",
    "        bar.update(11)\n",
    "\n",
    "        # compute expected features\n",
    "        expected_features_D_xy = expected_feature_vector(ref_forest, I_ref, O_ref, ref_edge2fmap)\n",
    "        bar.update(12)\n",
    "        \n",
    "        # update w\n",
    "        w_new, nabla_total = update_w(w, expected_features_D_xy, expected_features_Dn_x, delta=delta)\n",
    "        bar.update(13)\n",
    "\n",
    "        w = w_new\n",
    "        bar.finish()\n",
    "\n",
    "        d = viterbi(target_forest, tgt_tsort, tgt_edge_weights, I_tgt, root_tgt) # use exp!\n",
    "        candidates = write_derrivation(d)\n",
    "        print(\"Best y = '{}'\".format(candidates.pop()))\n",
    "        print('P(y,d|x) = {}'.format(joint_prob(d, tgt_edge_weights, I_tgt, root_tgt))) # use\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def joint_prob(derrivation, estimated_weights, inside, root):\n",
    "    \"\"\"\n",
    "    Computes the joint probability of a a sentence and its derrivation.\n",
    "    \"\"\"\n",
    "    numerator = np.exp(sum([np.log(estimated_weights[edge]) for edge in derrivation]))\n",
    "    Z = inside[root]\n",
    "    return numerator / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "N/A% (0 of 13) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y = 'the the dog black'\n",
      "P(y,d|x) = 5.6893074980508956e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "N/A% (0 of 13) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y = 'the dog black'\n",
      "P(y,d|x) = 0.06311787343618726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "N/A% (0 of 13) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y = 'the dog black'\n",
      "P(y,d|x) = 0.06311787343618726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "N/A% (0 of 13) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y = 'the dog black'\n",
      "P(y,d|x) = 0.06311787343618726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y = 'the dog black'\n",
      "P(y,d|x) = 0.06311787343618726\n"
     ]
    }
   ],
   "source": [
    "w_init = defaultdict(float)\n",
    "for feature in fset:\n",
    "    w_init[feature] = 0.01*np.random.uniform()\n",
    "w_test = sgd_func(5, 0.01, w_init, target_forest, ref_forest, src_fsa, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 0.339644069552\n",
      "span:lhs:src:3 -242.310009901\n",
      "span:lhs:tgt:3 -8.27550845038\n",
      "type:binary -1.31674535477\n",
      "type:monotone -1.32255120756\n",
      "span:rhs:src-lc:1 -0.98043545535\n",
      "span:rhs:src-rc:2 -1.34603581597\n",
      "span:rhs:tgt-lc:0 -0.215293861632\n",
      "span:rhs:tgt-rc:3 0.00708555517364\n",
      "type:inverted -1.32304243941\n",
      "span:rhs:tgt-lc:2 0.00876599015752\n",
      "span:rhs:tgt-rc:1 -1.31985806446\n",
      "type:deletion-slc -1.3183715301\n",
      "span:rhs:src-lc:0 -1.31760264275\n",
      "span:rhs:src-rc:3 -1.78274884\n",
      "span:rhs:tgt-lc:1 -1.31659681065\n",
      "span:rhs:tgt-rc:2 0.000137932466083\n",
      "span:rhs:src-lc:2 -1.34159422451\n",
      "span:rhs:src-rc:1 -0.982794087279\n",
      "span:rhs:tgt-lc:3 0.00122735528093\n",
      "span:rhs:tgt-rc:0 -3.86909719928\n",
      "type:terminal -15.4610406697\n",
      "type:deletion -45.0097030504\n",
      "del:le -45.0108147045\n",
      "span:rhs:src:1 -1.81592725594\n",
      "span:rhs:tgt:0 -45.0105104179\n",
      "span:lhs:src:1 -1.81156755447\n",
      "span:lhs:tgt:0 -45.0028041471\n",
      "type:translation -1.81407245987\n",
      "trans:le/the -1.81554771231\n",
      "span:rhs:tgt:1 -15.4594418747\n",
      "span:lhs:tgt:1 -15.4643383349\n",
      "type:deletion-src -1.32352538216\n",
      "span:rhs:src-rc:0 -1.32465955199\n",
      "type:insertion -15.4641684582\n",
      "ins:the -15.4674476668\n",
      "span:rhs:src:0 -15.4670522424\n",
      "span:lhs:src:0 -15.4656648071\n",
      "trans:chien/dog -47.4261036594\n",
      "trans:noir/black -31.6384327235\n",
      "span:lhs:tgt:2 -0.195589630546\n",
      "span:lhs:tgt:4 -242.302664396\n",
      "span:rhs:src-lc:3 -1.78103356601\n",
      "span:rhs:tgt-rc:4 0.00332138975487\n",
      "span:rhs:tgt-lc:4 0.00637549527017\n",
      "trans:noir/dark -31.6446824\n",
      "ins:a -15.4648680054\n"
     ]
    }
   ],
   "source": [
    "for k,v in w_test.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from itertools import chain\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Class used to run sgd on a corpus.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_init, parse_dict=dict()):\n",
    "        self.w = w_init\n",
    "        self.parse_dict = parse_dict\n",
    "    \n",
    "    \n",
    "    def update_w(self, wmap, expected_features_D_xy, expected_features_Dn_x, delta=0.1):\n",
    "        w_new = defaultdict(float)\n",
    "\n",
    "        for rule in chain(expected_features_D_xy, expected_features_Dn_x):\n",
    "            for feature in chain(expected_features_D_xy[rule], expected_features_Dn_x[rule]):\n",
    "                w_new[feature] = wmap[feature] + delta * (expected_features_D_xy[rule][feature] - \n",
    "                                                          expected_features_Dn_x[rule][feature])\n",
    "                \n",
    "        return w_new\n",
    "\n",
    "    def epoch(self, minibatch, delta, \n",
    "              parses=[], sparse=False):\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient ascent on the weights vector w.\n",
    "        \"\"\"\n",
    "        w = defaultdict(float)\n",
    "        for i, (src_sent, tgt_sent) in enumerate(minibatch):\n",
    "            \n",
    "            if not parses:\n",
    "                # get all required forests and fsa's\n",
    "                target_forest, ref_forest, src_fsa = parse_forests(src_sent, tgt_sent, lexicon)\n",
    "            \n",
    "            else:\n",
    "                target_forest, ref_forest, src_fsa = parses[i]\n",
    "                \n",
    "            # make sure i is the sentence index! not the minibatch index\n",
    "            #target_forest, ref_forest, src_fsa = load_parses_separate('../parses/', i) \n",
    "                        \n",
    "            ### D_n(x) ###\n",
    "\n",
    "            tgt_edge2fmap, _ = featurize_edges(target_forest, src_fsa,\n",
    "                                               sparse_del=sparse, sparse_ins=sparse, sparse_trans=sparse)\n",
    "\n",
    "            # recompute edge weights\n",
    "            tgt_edge_weights = {edge: weight_function(edge, tgt_edge2fmap[edge], self.w) \n",
    "                                for edge in target_forest}\n",
    "\n",
    "            # compute inside and outside\n",
    "            tsort = top_sort(target_forest)\n",
    "            root = Nonterminal(\"D_n(x)\")\n",
    "            I_tgt = inside_algorithm(target_forest, tsort, tgt_edge_weights)\n",
    "            O_tgt = outside_algorithm(target_forest, tsort, tgt_edge_weights, I_tgt, root)\n",
    "\n",
    "            # compute expected features\n",
    "            expected_features_Dn_x = expected_feature_vector(target_forest, I_tgt, O_tgt, tgt_edge2fmap)\n",
    "\n",
    "            \n",
    "            ### D(x,y) ###\n",
    "\n",
    "            ref_edge2fmap, _ = featurize_edges(ref_forest, src_fsa,\n",
    "                                               sparse_del=sparse, sparse_ins=sparse, sparse_trans=sparse)\n",
    "\n",
    "            # recompute edge weights\n",
    "            ref_edge_weights = {edge: weight_function(edge, ref_edge2fmap[edge], self.w) \n",
    "                                for edge in ref_forest}\n",
    "\n",
    "\n",
    "            # compute inside and outside\n",
    "            tsort = top_sort(ref_forest)\n",
    "            root = Nonterminal(\"D(x,y)\")\n",
    "            I_ref = inside_algorithm(ref_forest, tsort, ref_edge_weights)\n",
    "            O_ref = outside_algorithm(ref_forest, tsort, ref_edge_weights, I_ref, root)\n",
    "\n",
    "            # compute expected features\n",
    "            expected_features_D_xy = expected_feature_vector(ref_forest, I_ref, O_ref, ref_edge2fmap)\n",
    "\n",
    "            # optain new w\n",
    "            w_new = update_w(w, expected_features_D_xy, expected_features_Dn_x, delta=delta)\n",
    "            \n",
    "            for feature, value in w_new.items():\n",
    "                w[feature] += 1.0 / len(minibatch) * value\n",
    "        # update w\n",
    "        self.w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 3) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-744-1923ba1b0d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-743-7ac875eb32e9>\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(self, minibatch, delta, parses, sparse)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# compute inside and outside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mtsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D_n(x)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mI_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minside_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_edge_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-592-78228e9e4d13>\u001b[0m in \u001b[0;36mtop_sort\u001b[0;34m(forest)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutgoing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# outgoing = FS(u)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutgoing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-592-78228e9e4d13>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutgoing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# outgoing = FS(u)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutgoing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daan/Documents/Logic/NLP2/part2/Machine-Translation-with-CRFs/libitg.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daan/Documents/Logic/NLP2/part2/Machine-Translation-with-CRFs/libitg.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daan/Documents/Logic/NLP2/part2/Machine-Translation-with-CRFs/libitg.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_init = defaultdict(float)\n",
    "for feature in fset:\n",
    "    w_init[feature] = 0.06\n",
    "sgd = SGD(w_init)\n",
    "\n",
    "fr_sent = 'le chien noir'\n",
    "en_sent = 'the black dog'\n",
    "minibatch = [(fr_sent, en_sent)]\n",
    "lexicon = make_lexicon_ALT(fr_sent, en_sent)\n",
    "parses = parse_forests(fr_sent, en_sent, lexicon)\n",
    "\n",
    "n = 3\n",
    "bar = progressbar.ProgressBar(max_value=n)\n",
    "for i in range(n):\n",
    "    bar.update(i)\n",
    "    sgd.epoch(minibatch, 0.06, parses=[parses])\n",
    "    bar.update(i+1)\n",
    "\n",
    "w_new = sgd.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 2.7077020112348422e-173\n",
      "span:lhs:src:3 -6.433031916918697e-228\n",
      "span:lhs:tgt:3 -2.988060833832837e-172\n",
      "type:binary -1.537226172264425e-279\n",
      "type:monotone -1.537226172264425e-279\n",
      "span:rhs:src-lc:1 -5.1916922859004495e-224\n",
      "span:rhs:src-rc:2 -3.677435003397298e-289\n",
      "span:rhs:tgt-lc:0 -4.0329128633699385e-285\n",
      "span:rhs:tgt-rc:3 0.0\n",
      "type:inverted -1.537226172264425e-279\n",
      "span:rhs:tgt-lc:2 0.0\n",
      "span:rhs:tgt-rc:1 -1.537226172264425e-279\n",
      "type:deletion-slc -1.537226172264425e-279\n",
      "span:rhs:src-lc:0 -1.537226172264425e-279\n",
      "span:rhs:src-rc:3 -3.819921718785697e-284\n",
      "span:rhs:tgt-lc:1 -1.537226172264425e-279\n",
      "span:rhs:tgt-rc:2 0.0\n",
      "span:rhs:src-lc:2 -3.677435003397299e-289\n",
      "span:rhs:src-rc:1 -5.1916922859004495e-224\n",
      "span:rhs:tgt-lc:3 0.0\n",
      "span:rhs:tgt-rc:0 -1.2591361661413968e-223\n",
      "type:terminal -2.301228817854101e-223\n",
      "type:deletion -1.1490500340654827e-172\n",
      "span:rhs:src:1 -5.569582552086917e-173\n",
      "span:rhs:tgt:0 -1.1490500340654827e-172\n",
      "span:lhs:src:1 -5.569582552086917e-173\n",
      "span:lhs:tgt:0 -1.1490500340654827e-172\n",
      "type:translation -5.569582552086917e-173\n",
      "span:rhs:tgt:1 -2.301228817854101e-223\n",
      "span:lhs:tgt:1 -2.301228817854101e-223\n",
      "type:deletion-src -1.537226172264425e-279\n",
      "span:rhs:src-rc:0 -1.537226172264425e-279\n",
      "type:insertion -2.301228817854101e-223\n",
      "span:rhs:src:0 -2.301228817854101e-223\n",
      "span:lhs:src:0 -2.301228817854101e-223\n",
      "span:lhs:tgt:2 -8.985139123294071e-172\n",
      "span:lhs:tgt:4 -6.433031916918697e-228\n",
      "span:rhs:src-lc:3 -3.819921718785697e-284\n",
      "span:rhs:tgt-rc:4 0.0\n",
      "span:rhs:tgt-lc:4 0.0\n"
     ]
    }
   ],
   "source": [
    "for k,v in w_new.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Finally, at prediction time we have a big problem, because\n",
    "\\begin{align}\n",
    "y^\\star =    \\arg,\\max_y P(y|x, n)\n",
    "\\end{align}\n",
    "is intractable as proven by [Sima'an (1996)](http://www.aclweb.org/anthology/C/C96/C96-2215.pdf).\n",
    "Thus we typically approximate that criterion by\n",
    "\\begin{align}\n",
    "y^\\star =  \\text{yield}_\\Delta \\left(  \\arg,\\max_d P(y, d|x, n) \\right)\n",
    "\\end{align}\n",
    "which is often termed *Viterbi decoding*.\n",
    "\n",
    "All this takes is a forest respresenting all translation candidates, i.e. \\\\(\\mathcal D_n(x)\\\\) and a the Viterbi algorithm. Because in this project you need to implement *inside*, there's little point in programming Viterbi, you can simply use Inside for that. Check our [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(forest: CFG, tsort: list, edge_weights: dict, inside: dict, root: Symbol) -> dict:\n",
    "    \"\"\"Returns the outside weight of each node\"\"\"\n",
    "    Q = deque([root])\n",
    "    V = list()\n",
    "    while Q:\n",
    "        symbol = Q.popleft()\n",
    "        incoming = forest.get(symbol)\n",
    "        weights = [0.0]*len(incoming)\n",
    "        for i, edge in enumerate(incoming):\n",
    "            weights[i] = edge_weights[edge]\n",
    "            for u in edge.rhs: # u in tail(e)\n",
    "                weights[i] *= inside[u] # TODO: change to log-sum-exp\n",
    "        weight, selected = max(zip(weights, incoming), key=lambda xy: xy[0])\n",
    "        for sym in selected.rhs:\n",
    "            if not sym.is_terminal():\n",
    "                Q.append(sym)\n",
    "        V.append(selected)    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_derrivation(d):\n",
    "    derivation_as_fsa = libitg.forest_to_fsa(CFG(d), d[0].lhs)\n",
    "    candidates = libitg.enumerate_paths_in_fsa(derivation_as_fsa)\n",
    "    return candidates\n",
    "candidates = write_derrivation(d)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_nltk_tree(derivation):\n",
    "    \"\"\"return a nlt Tree object based on the derivation (list or tuple of Rules).\"\"\"\n",
    "    d = defaultdict(None, ((r.lhs, r.rhs) for r in derivation))\n",
    "    \n",
    "    def make_tree(lhs):\n",
    "        return Tree(lhs.obj(), (child if child not in d else make_tree(child) for child in d[lhs]))\n",
    "    \n",
    "    return make_tree(derivation[0].lhs)\n",
    "\n",
    "t = make_nltk_tree(d)\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import nan\n",
    "import string\n",
    "\n",
    "def preprocess_lexicon(path='data/lexicon', k=5, null=5, remove_punct=True):\n",
    "    f = open(path, 'r')\n",
    "    \n",
    "    ch_en_ = defaultdict(lambda: defaultdict(float))\n",
    "    en_ch_ = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for line in f:\n",
    "        ch, en, p_en_given_ch, p_ch_given_en = line.split()\n",
    "        # for use in the parsing we replace <NULL> with -EPS-\n",
    "        if ch == '<NULL>':\n",
    "            ch = '-EPS-' \n",
    "        if en == '<NULL>':\n",
    "            en = '-EPS-' \n",
    "        ch_en_[ch][en] = float(p_en_given_ch) if not p_en_given_ch == 'NA' else nan # perhaps something tiny like 10e-20\n",
    "        en_ch_[en][ch] = float(p_ch_given_en) if not p_ch_given_en == 'NA' else nan # perhaps something tiny like 10e-20\n",
    "    f.close()\n",
    "    \n",
    "    ch_en = defaultdict(lambda: defaultdict(float))\n",
    "    for ch in ch_en_.keys():\n",
    "        en_punct = string.punctuation\n",
    "        srtd = sorted(ch_en_[ch].items(), key=lambda xy: xy[1])\n",
    "        if ch == '-EPS-':\n",
    "            if remove_punct:\n",
    "                # when we do not want to insert punctuation from EPS\n",
    "                srtd = [(en, p) for en, p in srtd if en not in en_punct]\n",
    "            ch_en['-EPS-'] = {en: p for en, p in srtd[-null:]}\n",
    "        else:\n",
    "            ch_en[ch] = {en: p for en, p in srtd[-k:]}\n",
    "\n",
    "\n",
    "    en_ch = defaultdict(lambda: defaultdict(float))\n",
    "    for en in en_ch_.keys():\n",
    "        ch_punct = \"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？? 、~@#￥%……&*（）：；《）《》“”()»〔〕-]+\"\n",
    "        srtd = sorted(en_ch_[en].items(), key=lambda xy: xy[1])\n",
    "        if en == '-EPS-=':\n",
    "            if remove_punct:\n",
    "                # when we do not want to insert punctuation from EPS\n",
    "                srtd = [(ch, p) for ch, p in srtd if ch not in ch_punct]\n",
    "            en_ch['-EPS-'] = {ch: p for ch, p in srtd[-null:]}\n",
    "        else:\n",
    "            en_ch[en] = {ch: p for ch, p in srtd[-k:]}\n",
    "            \n",
    "    full_en_ch = en_ch_\n",
    "    full_ch_en = ch_en_\n",
    "    return ch_en, en_ch, full_en_ch, full_ch_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_en, en_ch, full_en_ch, full_ch_en = preprocess_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13960\n",
      "10512\n",
      "{'开课': 0.030781041830778122, '春卷': 0.052335865795612335, '明年': 0.05507362633943558, '是': 0.11269595474004745, '春天': 0.5166555643081665}\n",
      "{'it': 0.06706574559211731, 'to': 0.07063718140125275, 'a': 0.08496548235416412, 'i': 0.10233193635940552, 'the': 0.11601653695106506}\n",
      "{'了': 0.02192385494709015, '?': 0.04991278424859047, '我': 0.10082501918077469, '的': 0.25213390588760376, '。': 0.45774778723716736}\n",
      "nan\n",
      "3.0006491645135287e-31\n"
     ]
    }
   ],
   "source": [
    "print(len(ch_en))\n",
    "print(len(en_ch))\n",
    "print(en_ch['spring'])\n",
    "print(ch_en['-EPS-'])\n",
    "print(en_ch['-EPS-'])\n",
    "\n",
    "print(full_en_ch['the']['-EPS-']) # deletion of 'however' (given 'however' produce EPS)\n",
    "print(full_en_ch['-EPS-']['春天']) # insertion of '春天' (given EPS produce '春天')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data_dev(path='data/dev1.zh-en', max_sents=5):\n",
    "    f = open(path, 'r')\n",
    "    corpus = dict()\n",
    "    for k, line in enumerate(f):\n",
    "        if k + 1 > max_sents:\n",
    "            break\n",
    "        sents = line[:-1].split('|||')\n",
    "        ch = sents[0]\n",
    "        translations = list()\n",
    "        for en in sents[1:]:\n",
    "            translations.append(en)\n",
    "        corpus[ch] = translations\n",
    "    return corpus\n",
    "\n",
    "def read_data(path='data/training.zh-en', max_sents=5):\n",
    "    f = open(path, 'r')\n",
    "    corpus = list()\n",
    "    for k, line in enumerate(f):\n",
    "        if k + 1 > max_sents:\n",
    "            break\n",
    "        ch, en = line[:-1].split('|||') # line[:-1] to remove '\\n' character\n",
    "        corpus.append((ch, en))\n",
    "    return corpus\n",
    "\n",
    "def make_lexicon(ch_sent, ch_en, en_ch):\n",
    "    \"\"\"\n",
    "    Given a chinese sentence produces a lexicon of possible translation as dictionary\n",
    "    Format: chinese character -> {top 5 english translations}\n",
    "    :param ch_sent: a chinese sentence as string (e.g. '在 门厅 下面 。 ')\n",
    "    :param ch_sent: a chinese sentence as string (e.g. 'it 's just down the hall .')\n",
    "    \"\"\"\n",
    "    lexicon = defaultdict(set)\n",
    "    lexicon['-EPS-'].update(ch_en['-EPS-'])\n",
    "    for char in ch_sent.split():\n",
    "        lexicon[char].update(ch_en[char])\n",
    "    return lexicon\n",
    "\n",
    "def make_lexicon_ALT(ch_sent, en_sent):\n",
    "    \"\"\"\n",
    "    Given a chinese sentence produces a lexicon of possible translation as dictionary\n",
    "    Format: chinese character -> {all english words in training sentence}\n",
    "    \"\"\"\n",
    "    lexicon = defaultdict(set)\n",
    "    lexicon['-EPS-'].update(en_sent.split())\n",
    "    for char in ch_sent.split():\n",
    "        lexicon[char].update(en_sent.split())\n",
    "        lexicon[char].add('-EPS-')\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('在 门厅 下面 。 ', \" it 's just down the hall .\"), ('我 这 就 给 您 拿 一些 。 ', \" i 'll bring you some now .\"), ('如果 您 还 有 什么 需要 , 尽管 告诉 我 。 ', ' if there is anything else you need , just let me know .'), ('不用 担心 那个 。 ', ' no worry about that .'), ('我 要 买 它 , 你 不 需要 把 它 包 起来 。 ', \" i 'll take it and you need not wrap it up .\")]\n",
      "defaultdict(<class 'set'>, {'-EPS-': {'it', 'the', 'i', 'a', 'to'}, '一': {'one', '.', 'of', '-EPS-', 'a'}, '跳': {'it', \"'s\", 'pain', '-EPS-', 'throbbing'}, '的': {'the', '.', 'of', '-EPS-', 'a'}, '痛': {'i', 'pain', '-EPS-', \"'ve\", 'hurts'}, '。': {',', 'i', '.', '-EPS-', 'a'}})\n"
     ]
    }
   ],
   "source": [
    "corpus = read_data()\n",
    "print(corpus)\n",
    "ch_sent = '一 跳 一 跳 的 痛 。 '\n",
    "lexicon = make_lexicon(ch_sent, ch_en, en_ch)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    ch_sent, en_sent = corpus[1]\n",
    "    print(ch_sent)\n",
    "    print(en_sent)\n",
    "#     lexicon = make_lexicon(ch_sent, ch_en, en_ch)\n",
    "    lexicon = make_lexicon_ALT(ch_sent, en_sent)\n",
    "\n",
    "    print(lexicon)\n",
    "    src_fsa = libitg.make_fsa(ch_sent)\n",
    "    print(src_fsa)\n",
    "    src_cfg = libitg.make_source_side_itg(lexicon)\n",
    "    print(src_cfg)\n",
    "    forest = libitg.earley(src_cfg, src_fsa, \n",
    "                           start_symbol=Nonterminal('S'), \n",
    "                           sprime_symbol=Nonterminal(\"D(x)\"))\n",
    "    print('forest: {}'.format(len(forest)))\n",
    "\n",
    "    projected_forest = libitg.make_target_side_itg(forest, lexicon)\n",
    "    print('projected forest: {}'.format(len(projected_forest)))\n",
    "    \n",
    "    tgt_fsa = libitg.make_fsa(en_sent)\n",
    "    print(tgt_fsa)\n",
    "    ref_forest = libitg.earley(projected_forest, tgt_fsa, \n",
    "                               start_symbol=Nonterminal(\"D(x)\"), \n",
    "                               sprime_symbol=Nonterminal('D(x,y)'))\n",
    "    print('ref forest: {}'.format(len(ref_forest)))\n",
    "\n",
    "    print('pass1')\n",
    "    length_fsa = libitg.LengthConstraint(len(en_sent), strict=False)\n",
    "    print('pass2')\n",
    "\n",
    "    target_forest = libitg.earley(projected_forest, length_fsa, \n",
    "                                  start_symbol=Nonterminal(\"D(x)\"), \n",
    "                                  sprime_symbol=Nonterminal(\"D_n(x)\"))\n",
    "    print('target forest: {}'.format(len(target_forest)))\n",
    "    print('pass3')\n",
    "\n",
    "    return target_forest, ref_forest, src_fsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 这 就 给 您 拿 一些 。 \n",
      " i 'll bring you some now .\n",
      "defaultdict(<class 'set'>, {'-EPS-': {'i', '.', 'some', \"'ll\", 'you', 'now', 'bring'}, '我': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '这': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '就': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '给': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '您': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '拿': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '一些': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}, '。': {'i', '.', 'some', \"'ll\", '-EPS-', 'you', 'now', 'bring'}})\n",
      "states=9\n",
      "initial=0\n",
      "final=8\n",
      "arcs=8\n",
      "origin=0 destination=1 label=我\n",
      "origin=1 destination=2 label=这\n",
      "origin=2 destination=3 label=就\n",
      "origin=3 destination=4 label=给\n",
      "origin=4 destination=5 label=您\n",
      "origin=5 destination=6 label=拿\n",
      "origin=6 destination=7 label=一些\n",
      "origin=7 destination=8 label=。\n",
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| '-EPS-'\n",
      "[X] ||| '我'\n",
      "[X] ||| '这'\n",
      "[X] ||| '就'\n",
      "[X] ||| '给'\n",
      "[X] ||| '您'\n",
      "[X] ||| '拿'\n",
      "[X] ||| '一些'\n",
      "[X] ||| '。'\n",
      "forest: 184\n",
      "projected forest: 450\n",
      "states=8\n",
      "initial=0\n",
      "final=7\n",
      "arcs=7\n",
      "origin=0 destination=1 label=i\n",
      "origin=1 destination=2 label='ll\n",
      "origin=2 destination=3 label=bring\n",
      "origin=3 destination=4 label=you\n",
      "origin=4 destination=5 label=some\n",
      "origin=5 destination=6 label=now\n",
      "origin=6 destination=7 label=.\n",
      "ref forest: 32953\n",
      "pass1\n",
      "pass2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-0324f31b4bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mch_target_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_ref_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_src_fsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-145-c2fa9fa06f91>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     target_forest = libitg.earley(projected_forest, length_fsa, \n\u001b[1;32m     33\u001b[0m                                   \u001b[0mstart_symbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D(x)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                   sprime_symbol=Nonterminal(\"D_n(x)\"))\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target forest: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pass3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daan/Documents/Logic/NLP2/part2/Machine-Translation-with-CRFs/libitg.py\u001b[0m in \u001b[0;36mearley\u001b[0;34m(cfg, fsa, start_symbol, sprime_symbol, eps_symbol)\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# we have already predicted in this context, let's attempt completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0mconsequents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantecedent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsequents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# mark this antecedent as processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ch_target_forest, ch_ref_forest, ch_src_fsa = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch_target_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-6e1601d65066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mw_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.06\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_target_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_ref_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_src_fsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ch_target_forest' is not defined"
     ]
    }
   ],
   "source": [
    "w_init = defaultdict(float)\n",
    "for feature in fset:\n",
    "    w_init[feature] = 0.06\n",
    "w_new = sgd_func(1, 0.6, w_init, ch_target_forest, ch_ref_forest, ch_src_fsa, sparse=False)\n",
    "\n",
    "print(w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_edge2fmap, _ = featurize_edges(ch_target_forest, ch_src_fsa,\n",
    "                                  sparse_del=False, sparse_ins=False, sparse_trans=False)\n",
    "\n",
    "ch_estimated_edge_weights = {edge: weight_function(edge, ch_edge2fmap[edge], w_new) for edge in ch_target_forest}\n",
    "ch_tsort = top_sort(ch_target_forest)\n",
    "ch_I = inside_algorithm(ch_ref_forest, ch_tsort, ch_estimated_edge_weights)\n",
    "ch_d = viterbi(ch_target_forest, ch_I, Span(Nonterminal('D(x)'), 0, 2), ch_estimated_edge_weights)\n",
    "ch_tsort = top_sort(ch_target_forest)\n",
    "ch_d = Viterbi(ch_target_forest, ch_tsort, ch_estimated_edge_weights, I, Span(Nonterminal('D(x)'), 0, 2))\n",
    "write(ch_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "import pickle\n",
    "\n",
    "def parse_forests(src_sent, tgt_sent, lexicon):\n",
    "    \"\"\"\n",
    "    Parses the forests needed for epoch\n",
    "    \"\"\"\n",
    "    src_fsa = libitg.make_fsa(src_sent)\n",
    "    src_cfg = libitg.make_source_side_itg(lexicon)\n",
    "    forest = libitg.earley(src_cfg, src_fsa, \n",
    "                           start_symbol=Nonterminal('S'), \n",
    "                           sprime_symbol=Nonterminal(\"D(x)\"))\n",
    "    \n",
    "    projected_forest = libitg.make_target_side_itg(forest, lexicon)\n",
    "    \n",
    "    tgt_fsa = libitg.make_fsa(tgt_sent)\n",
    "    ref_forest = libitg.earley(projected_forest, tgt_fsa, \n",
    "                               start_symbol=Nonterminal(\"D(x)\"), \n",
    "                               sprime_symbol=Nonterminal('D(x,y)'),\n",
    "                               eps_symbol=Nonterminal('-EPS-'))\n",
    "    \n",
    "    length_fsa = libitg.LengthConstraint(len(tgt_sent), strict=False)\n",
    "    target_forest = libitg.earley(projected_forest, length_fsa, \n",
    "                                  start_symbol=Nonterminal(\"D(x)\"), \n",
    "                                  sprime_symbol=Nonterminal(\"D_n(x)\"))\n",
    "    \n",
    "    return target_forest, ref_forest, src_fsa\n",
    "\n",
    "\n",
    "def save_parses(corpus, savepath):\n",
    "    \"\"\"\n",
    "    Parses all sentences in corpus and saves a triple of needed ones in (huge) dictionary\n",
    "    indexed by sentence number in corpus as pkl object at savepath.\n",
    "\n",
    "    :corpus: a list of tuples [(chinese sentence, english sentence)] \n",
    "    :saves: parse_dict: sentence number -> (target_forest, ref_forest, scr_fsa)   \n",
    "    \"\"\"\n",
    "    parse_dict = dict() \n",
    "    for i, (ch_sent, en_sent) in enumerate(corpus):\n",
    "        lexicon = make_lexicon_ALT(ch_sent, en_sent)\n",
    "\n",
    "        parses = parse_forests(ch_sent, en_sent, lexicon)\n",
    "        \n",
    "        parse_dict[i] = parses\n",
    "    \n",
    "    f = open(savepath + 'parse_dict.pkl', 'wb')\n",
    "    pickle.dump(parse_dict, f, protocol=4)\n",
    "    f.close()\n",
    "\n",
    "def save_parses_separate(corpus, savepath):\n",
    "    \"\"\"\n",
    "    For each sentence k in corpus we parse and save the triple of needed parses \n",
    "    as pkl object at savepath/parses-k.pkl.\n",
    "\n",
    "    :corpus: a list of tuples [(chinese sentence, english sentence)] \n",
    "    :saves: parses-k = (target_forest, ref_forest, scr_fsa) for each k in 0,..,len(corpus)\n",
    "    \"\"\"\n",
    "    for k, (ch_sent, en_sent) in enumerate(corpus):\n",
    "        lexicon = make_lexicon_ALT(ch_sent, en_sent)\n",
    "\n",
    "        parses = parse_forests(ch_sent, en_sent, lexicon)\n",
    "        \n",
    "        f = open(savepath + 'parses-{}.pkl'.format(k), 'wb')\n",
    "        pickle.dump(parses, f, protocol=4)\n",
    "        f.close()\n",
    "    \n",
    "def load_parses(savepath):\n",
    "    \"\"\"\n",
    "    Loads and returns a parse_dict as saved by load_parses.\n",
    "    \"\"\"\n",
    "    f = open(savepath + 'parse_dict.pkl', 'rb')\n",
    "    parse_dict = pickle.load(f)\n",
    "    f.close()\n",
    "    return parse_dict\n",
    "\n",
    "def load_parses_separate(savepath, k):\n",
    "    \"\"\"\n",
    "    Loads and returns parses as saved by load_parses_separate\n",
    "    \"\"\"\n",
    "    f = open(savepath + 'parses-{k}.pkl', 'rb')\n",
    "    parses = pickle.load(f)\n",
    "    f.close()\n",
    "    return parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_parses(corpus[:1], '')\n",
    "\n",
    "parse_dict = load_parses('')\n",
    "print(parse_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_parses_separate(corpus[:3], '../parses/')\n",
    "\n",
    "# parses = load_parses_separate('../parses/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_init = defaultdict(float)\n",
    "for feature in fset:\n",
    "    w_init[feature] = 0.06\n",
    "sgd = SGD(w_init, parse_dict)\n",
    "\n",
    "for i in range(1):\n",
    "    sgd.epoch([('le chien noir', 'the black dog')], 0.6, sparse=False)\n",
    "\n",
    "w_new = sgd.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
