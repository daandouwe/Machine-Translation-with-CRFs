Some notes on training:

* Use a large batch size. Probably in the range 30-100. This gives stability to the updates of w, since most of the features don’t ‘fire’ for one training example.

* Using shuffle=True we reshuffle the parses and partition these into new minibatches at each iteration. This drastically improves ‘movement’: compare the sentences in [shuffle](prediction/2k/shuffle) to those in [no-shuffle](prediction/2k/no-shuffle) and see the difference!

* When we shuffle, we should let the learning rate decay more rapidly. For example delta_0=10 and lmbda=50. Then we start large, but decay rapidly.